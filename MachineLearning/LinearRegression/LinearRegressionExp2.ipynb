{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self,x,y,w):\n",
    "        x = np.concatenate((np.ones((x.shape[0], 1, 1)), x), axis=1)\n",
    "        print(x[0].shape)\n",
    "        print(w.shape)\n",
    "        self.input = x\n",
    "        self.real = y\n",
    "        print(self.real.shape)\n",
    "        self.weight = w\n",
    "        self.m = x.shape[0]\n",
    "    def predict(self):\n",
    "        self.pred = np.zeros(self.input.shape[0])\n",
    "        for i in range(self.input.shape[0]):\n",
    "            self.pred[i] = self.input[i].T.dot(self.weight)\n",
    "    def loss_func(self):\n",
    "        loss_arr = (self.pred-self.real)**2\n",
    "        return np.sum(loss_arr)/self.m\n",
    "    def gradient_decent(self, lr):\n",
    "        self.predict()\n",
    "        accumulated_weight = np.zeros(self.weight.shape, dtype=float)\n",
    "        for i in range(self.input.shape[0]):\n",
    "            accumulated_weight += ((self.pred[i] - self.real[i])*self.input[i])\n",
    "        accumulated_weight /= self.input.shape[0]\n",
    "        self.weight -= lr*accumulated_weight\n",
    "            # self.weight -= lr* ((self.pred[i] - self.real[i])*self.input[i]) / self.m\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n",
      "(4, 1)\n",
      "(200,)\n",
      "[[10.03350388]\n",
      " [ 1.99570638]\n",
      " [ 2.98799252]\n",
      " [-2.01076433]]\n",
      "[[1.        ]\n",
      " [1.58761169]\n",
      " [1.16585778]\n",
      " [1.38772663]]\n",
      "(200,) [-0.04234383 -0.25569101  0.03286629 -0.11662965  0.08856611  0.05099812\n",
      "  0.01910625 -0.11893759 -0.01462302 -0.04712225  0.00535476 -0.18582589\n",
      " -0.08237805 -0.07153294 -0.00735511 -0.01075477  0.14772407 -0.03334339\n",
      "  0.00635907 -0.0103231  -0.02013396  0.0381798   0.17818461 -0.19228872\n",
      " -0.03076134  0.01750556 -0.23053335  0.04331444 -0.04552277 -0.02841245\n",
      " -0.00070049  0.05308859  0.04723234  0.09936908 -0.03210889  0.01184353\n",
      "  0.12241333 -0.04452589  0.00967307 -0.10729114 -0.07939898 -0.13668512\n",
      " -0.20380855 -0.06412857  0.13460188  0.17408435 -0.0544068  -0.00068549\n",
      " -0.12129615  0.05778118 -0.01380481 -0.16373121 -0.14129198  0.07133816\n",
      " -0.0297199  -0.03511325  0.02203327 -0.10915911  0.016691   -0.17185381\n",
      " -0.01055376 -0.14869562  0.1285898   0.15998757  0.03162595  0.1231525\n",
      "  0.0124354   0.01622382  0.04213518 -0.04717883  0.04895001  0.10476759\n",
      " -0.06539102 -0.10424603 -0.0242482  -0.28861341  0.25798181  0.03612666\n",
      "  0.02176562 -0.10993386  0.03104716  0.1542675   0.2744943  -0.01381971\n",
      " -0.07622911  0.10780895  0.18034538 -0.04599081 -0.00118808  0.16571497\n",
      "  0.09620339  0.02157041  0.11066916  0.14052722 -0.06964421  0.04423008\n",
      " -0.09709478  0.13326144 -0.05055283  0.0566961   0.02178927  0.01270428\n",
      "  0.02358457 -0.00789689 -0.06188616  0.12880529  0.06095023  0.22918906\n",
      " -0.17393678  0.04202954  0.07140677  0.16586979  0.1447766   0.07604243\n",
      " -0.02144751  0.07206348  0.16048348 -0.06807986 -0.08736727 -0.10235355\n",
      " -0.02590474 -0.0678861  -0.02197597  0.01617061 -0.12478547 -0.13709299\n",
      "  0.11270293  0.0348543  -0.145851   -0.08822922  0.08171011  0.04174588\n",
      "  0.05580635 -0.12967467 -0.18051741 -0.19581766  0.02174738  0.10196266\n",
      " -0.07225706  0.13408371  0.12173028  0.10043143 -0.01257832 -0.02026539\n",
      "  0.13688668 -0.03805188 -0.0469122   0.10645173  0.00698326 -0.10067916\n",
      " -0.22060172  0.13960195  0.03863704  0.0015011  -0.0029997  -0.14582389\n",
      "  0.09618548  0.1436019  -0.10864729 -0.04942693  0.1254538  -0.01625859\n",
      "  0.13070896  0.12779734  0.12742477 -0.17603096 -0.27126103  0.10398072\n",
      "  0.1725768   0.21626335 -0.10705651  0.08721837  0.18747945  0.14234442\n",
      "  0.28279403  0.14369457 -0.02760012 -0.01083554 -0.08443015 -0.10516667\n",
      " -0.08888773  0.04343757 -0.07155549 -0.07781497 -0.0165835  -0.03724529\n",
      " -0.10210523  0.0324807  -0.120919   -0.08197439  0.02120288  0.01380921\n",
      " -0.17364267 -0.04975912  0.09570855 -0.18664075 -0.00627088 -0.04152925\n",
      " -0.02287428 -0.16275941]\n",
      "0.011933594453171312\n"
     ]
    }
   ],
   "source": [
    "m = 200\n",
    "dim = 3\n",
    "learning_rate = 1e-2\n",
    "x = np.random.rand(m,dim) * 2\n",
    "x = np.expand_dims(x, axis=2)\n",
    "y = x[:,0]*2 + x[:,1]*3 + x[:,2]*(-2) + 10\n",
    "y += np.random.randn(m,1)*0.1\n",
    "y = np.reshape(y, m)\n",
    "w = np.random.randn(dim+1,1)\n",
    "\n",
    "model = LinearRegression(x,y,w)\n",
    "for epoch in range(30000):\n",
    "    model.gradient_decent(learning_rate)\n",
    "print(model.weight)\n",
    "print(model.input[1])\n",
    "model.predict()\n",
    "error = model.pred-model.real\n",
    "print(error.shape, error)\n",
    "print(model.loss_func())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
